(self.webpackChunkcomet_docs=self.webpackChunkcomet_docs||[]).push([[3497],{3905:function(e,t,n){"use strict";n.d(t,{Zo:function(){return m},kt:function(){return u}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),d=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},m=function(e){var t=d(e.components);return a.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),c=d(n),u=r,f=c["".concat(s,".").concat(u)]||c[u]||p[u]||o;return n?a.createElement(f,l(l({ref:t},m),{},{components:n})):a.createElement(f,l({ref:t},m))}));function u(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,l=new Array(o);l[0]=c;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i.mdxType="string"==typeof e?e:r,l[1]=i;for(var d=2;d<o;d++)l[d]=n[d];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},2376:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return l},metadata:function(){return i},toc:function(){return s},default:function(){return m}});var a=n(2122),r=n(9756),o=(n(7294),n(3905)),l={sidebar_position:4,title:"Configuration"},i={unversionedId:"userguide/configuration",id:"userguide/configuration",isDocsHomePage:!1,title:"Configuration",description:"Environment variables",source:"@site/docs/userguide/configuration.md",sourceDirName:"userguide",slug:"/userguide/configuration",permalink:"/comet-data-pipeline/docs/userguide/configuration",editUrl:"https://github.com/ebiznext/comet-data-pipeline/edit/master/docs/docs/userguide/configuration.md",version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4,title:"Configuration"},sidebar:"cometSidebar",previous:{title:"Example",permalink:"/comet-data-pipeline/docs/userguide/sample"},next:{title:"Environment",permalink:"/comet-data-pipeline/docs/reference/environment"}},s=[{value:"Environment variables",id:"environment-variables",children:[]},{value:"Airflow DAGs",id:"airflow-dags",children:[{value:"Import DAG",id:"import-dag",children:[]},{value:"Watch DAG",id:"watch-dag",children:[]},{value:"Ingestion DAG",id:"ingestion-dag",children:[]}]}],d={toc:s};function m(e){var t=e.components,n=(0,r.Z)(e,["components"]);return(0,o.kt)("wrapper",(0,a.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"environment-variables"},"Environment variables"),(0,o.kt)("p",null,"By default, Comet expect metadata in the /tmp/metadata folder and will store ingested datasets in the /tmp/datasets folder.\nBelow is how the folders look like by default for the provided quickstart sample."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"    /tmp\n    |-- datasets (Root folder of ingested datasets)\n    |   |-- accepted (Root folder of all valid records)\n    |   |   |-- hr (domain name as specified in the name attribute of the /tmp/metadata/hr.yml)\n    |   |   |   `-- sellers (Schema name as specified in the /tmp/metadata/hr.yml)\n    |   |   |       |-- _SUCCESS\n    |   |   |       `-- part-00000-292c081b-7291-4797-b935-17bc9409b03b.snappy.parquet\n    |   |   `-- sales\n    |   |       |-- customers (valid records for this schema as specified in the /tmp/metadata/sales.yml)\n    |   |       |   |-- _SUCCESS\n    |   |       |   `-- part-00000-562501a1-34ef-4b94-b527-8e93bcbb5f89.snappy.parquet\n    |   |       `-- orders (valid records for this schema as specified in the /tmp/metadata/sales.yml)\n    |   |           |-- _SUCCESS\n    |   |           `-- part-00000-92544093-4ae2-4a98-8df8-a5aba19a1b27.snappy.parquet\n    |   |-- archive (Source files as found in the incoming folder are saved here after processing)\n    |   |   |-- hr (Domain name)\n    |   |   |   `-- sellers-2018-01-01.json\n    |   |   `-- sales\n    |   |       |-- customers-2018-01-01.psv\n    |   |       `-- orders-2018-01-01.csv\n    |   |-- business\n    |   |   |-- hr\n    |   |   `-- sales\n    |   |-- metrics\n    |   |   |-- discrete\n    |   |   |-- continuous\n    |   |   `-- frequencies\n    |   |-- ingesting (Temporary folder used during ingestion by Comet)\n    |   |   |-- hr (One temporary subfolder / domain)\n    |   |   `-- sales\n    |   |-- pending (Source files are copied here from the incoming folder before processing)\n    |   |   |-- hr (one folder / domain)\n    |   |   `-- sales\n    |   |-- rejected (invalid records in processed datasets are stored here)\n    |   |   |-- hr (Domain name)\n    |   |   |   `-- sellers (Schema name)\n    |   |   |       |-- _SUCCESS\n    |   |   |       `-- part-00000-aef2dde6-af24-4e20-ad88-3e5238916e57.snappy.parquet\n    |   |   `-- sales\n    |   |       |-- customers\n    |   |       |   |-- _SUCCESS\n    |   |       |   `-- part-00000-e6fa5ff9-ad29-4e5f-a5ff-549dd331fafd.snappy.parquet\n    |   |       `-- orders\n    |   |           |-- _SUCCESS\n    |   |           `-- part-00000-6f7ba5d4-960b-4ac6-a123-87a7ab2d212f.snappy.parquet\n    |   `-- unresolved (Files found in the incoming folder but do not match any schema)\n    |       `-- hr\n    |           `-- dummy.json\n    `-- metadata (Root of metadata files)\n        |-- domains (all domain definition files are located in this folder)\n        |   |-- hr.yml (One definition file / domain)\n        |   `-- sales.yml\n        `-- assertions (All assertion definitions go here)\n        |   |-- default.comet.yml (Predefined assertion definitions)\n        |   `-- assertions.comet.yml (assertion definitions defined here are accessible throughout the project)\n        `-- views (All views definitions go here)\n        |   |-- default.comet.yml (Predefined view definitions)\n        |   `-- views.comet.yml (view definitions defined here are accessible throughout the project)\n        `-- types (All semantic types are defined here)\n        |   |-- default.comet.yml (Default semantic types)\n        |   `-- types.comet.yml (User defined semantic types, overwrite default ones)\n        `-- jobs (All transform jobs go here)\n            `-- sales-by-name.yml (Compute sales by )\n")),(0,o.kt)("p",null,"Almost all options are customizable through environnement vairables.\nThe main env vars are described below, you may change default settings. The exhaustive list of predefined env vars can be found in the reference.conf file."),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:null},"Env. Var"),(0,o.kt)("th",{parentName:"tr",align:null},"Description"),(0,o.kt)("th",{parentName:"tr",align:null},"Default value"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_TMPDIR"),(0,o.kt)("td",{parentName:"tr",align:null},"When compacting data and estimating number of partitions, Comet stores intermediates files in this folder"),(0,o.kt)("td",{parentName:"tr",align:null},"hdfs:///tmp/comet_tmp")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_DATASETS"),(0,o.kt)("td",{parentName:"tr",align:null},"Once imported where the datasets are stored"),(0,o.kt)("td",{parentName:"tr",align:null},"eq. hdfs:///tmp/datasets")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_METADATA"),(0,o.kt)("td",{parentName:"tr",align:null},"Root folder where domains and types metadata are stored"),(0,o.kt)("td",{parentName:"tr",align:null},"/tmp/metadata")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_ARCHIVE"),(0,o.kt)("td",{parentName:"tr",align:null},"Should we archive the incoming files once they are ingested"),(0,o.kt)("td",{parentName:"tr",align:null},"true")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_LAUNCHER"),(0,o.kt)("td",{parentName:"tr",align:null},"Valid values are airflow or simple"),(0,o.kt)("td",{parentName:"tr",align:null},"simple")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_HIVE"),(0,o.kt)("td",{parentName:"tr",align:null},"Should be create external tables for ingested files?"),(0,o.kt)("td",{parentName:"tr",align:null},"true")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_ANALYZE"),(0,o.kt)("td",{parentName:"tr",align:null},"Should we computed basic statistics (required COMET_HIVE to be set to true) ?"),(0,o.kt)("td",{parentName:"tr",align:null},"true")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_WRITE_FORMAT"),(0,o.kt)("td",{parentName:"tr",align:null},"How ingested files are stored (parquet / orc / json / csv / avro)"),(0,o.kt)("td",{parentName:"tr",align:null},"parquet")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_AREA_PENDING"),(0,o.kt)("td",{parentName:"tr",align:null},"In $COMET_DATASET folder how the pending folder should be named"),(0,o.kt)("td",{parentName:"tr",align:null},"pending")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_AREA_UNRESOLVED"),(0,o.kt)("td",{parentName:"tr",align:null},"In $COMET_DATASET folder how the unresolved folder should be named"),(0,o.kt)("td",{parentName:"tr",align:null},"unresolved")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_AREA_ARCHIVE"),(0,o.kt)("td",{parentName:"tr",align:null},"In $COMET_DATASET folder how the archive folder should be named"),(0,o.kt)("td",{parentName:"tr",align:null},"archive")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_AREA_INGESTING"),(0,o.kt)("td",{parentName:"tr",align:null},"In $COMET_DATASET folder how the ingesting folder should be named"),(0,o.kt)("td",{parentName:"tr",align:null},"ingesting")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_AREA_ACCEPTED"),(0,o.kt)("td",{parentName:"tr",align:null},"In $COMET_DATASET folder how the accepted folder should be named"),(0,o.kt)("td",{parentName:"tr",align:null},"accepted")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_AREA_REJECTED"),(0,o.kt)("td",{parentName:"tr",align:null},"In $COMET_DATASET folder how the rejected folder should be named"),(0,o.kt)("td",{parentName:"tr",align:null},"rejected")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"COMET_AREA_BUSINESS"),(0,o.kt)("td",{parentName:"tr",align:null},"In $COMET_DATASET folder how the business folder should be named"),(0,o.kt)("td",{parentName:"tr",align:null},"business")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"AIRFLOW_ENDPOINT"),(0,o.kt)("td",{parentName:"tr",align:null},"Airflow endpoint. Used when COMET_LAUNCHER is set to airflow"),(0,o.kt)("td",{parentName:"tr",align:null},(0,o.kt)("a",{parentName:"td",href:"http://127.0.0.1:8080/api/experimental"},"http://127.0.0.1:8080/api/experimental"))))),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"\ud83d\udcdd ",(0,o.kt)("strong",{parentName:"p"},"When running on Cloudera 5.X.X prefer ORC to Parquet for the COMET_WRITE_FORMAT since Cloudera comes with Hive 1.1 which does\nnot support date/timestamp fields or else simply treat dates / timestamps as strings. See ",(0,o.kt)("a",{parentName:"strong",href:"https://issues.apache.org/jira/browse/HIVE-6394"},"HIVE_6394")))),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"\ud83d\udcdd ",(0,o.kt)("strong",{parentName:"p"},"When running Spark on YARN in cluster mode,\nenvironment variables need to be set using the syntax spark.yarn.appMasterEnv.","[EnvironmentVariableName]"))),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"\ud83d\udcdd ",(0,o.kt)("strong",{parentName:"p"},'When running Dataproc on GCP, environment variables need to be set\nin the DataprocClusterCreateOperator in the properties attributes\nusing the syntax "spark-env:',"[EnvironmentVariableName]",'":"',"[Value]",'"'))),(0,o.kt)("h2",{id:"airflow-dags"},"Airflow DAGs"),(0,o.kt)("p",null,"Comet Data Pipeline comes with native  Airflow support.\nBelow are DAG definitions for each of the three ingestion steps on an kerberized cluster."),(0,o.kt)("h3",{id:"import-dag"},"Import DAG"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\n\n\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2018, 11, 2),\n    'email': ['me@here.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5),\n\n}\n\ndag = DAG('comet_import',max_active_runs=1, catchup=False, default_args=default_args, schedule_interval='*/1 * * * *')\n\n\n\nCOMET_SPARK_CMD = \"spark2-submit \\\n                        --keytab /etc/keytabs/importhdfs.keytab \\\n                        --principal importhdfs@MY.BIGDATA \\\n                        --conf spark.jars.packages=\\\"\\\" \\\n                        --master yarn \\\n                        --deploy-mode client /home/airflow/program/comet-assembly-0.1.jar\"\n\nCometImport = BashOperator(\n    task_id='comet_import',\n    bash_command= COMET_SPARK_CMD + ' import',\n    env={\n        'COMET_DATASETS':\"/project/data\",\n        'COMET_METADATA':\"/project/metadata\",\n        'COMET_AREA_ACCEPTED':\"working\",\n        'COMET_AREA_PENDING':\"staging\",\n        'COMET_ARCHIVE':\"true\",\n        'COMET_LAUNCHER':\"airflow\",\n        'COMET_HIVE':\"true\",\n        'COMET_ANALYZE':\"true\"\n    },\n    dag=dag)\n")),(0,o.kt)("h3",{id:"watch-dag"},"Watch DAG"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import os\nfrom airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime, timedelta\nfrom airflow.operators.slack_operator import SlackAPIPostOperator\n\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2018, 11, 2),\n    'email': ['me@here.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5),\n    # 'queue': 'bash_queue',\n    # 'pool': 'backfill',\n    # 'priority_weight': 10,\n    # 'end_date': datetime(2016, 1, 1),\n}\n\ndag = DAG('comet_watcher',max_active_runs=1 , catchup=False, default_args=default_args, schedule_interval='*/1 * * * *')\n\ndef slack_task(msg):\n    slack_alert = SlackAPIPostOperator(\n        task_id='slack_alert',\n        channel=\"#airflow\",\n        token=\"xoxp-64071012534-475450904118-524897638692-f9a90d49fd7fb312a574b4570d557b9a\",\n        text = msg,\n        username = 'airflow',)\n    return slack_alert.execute(msg=msg)\n\nCOMET_SPARK_CMD = \"spark2-submit \\\n                        --keytab /etc/keytabs/importhdfs.keytab \\\n                        --principal importhdfs@MY.BIGDATA \\\n                        --conf spark.jars.packages=\\\"\\\" \\\n                        --master yarn \\\n                        --deploy-mode client /home/airflow/program/comet-assembly-0.1.jar\"\n\nCOMET_DOMAIN = os.environ.get('COMET_DOMAIN', '')\nCometWatch = BashOperator(\n    task_id='comet_watcher',\n    bash_command= COMET_SPARK_CMD + ' watch '+ COMET_DOMAIN,\n    #on_failure_callback=slack_task(\":red_circle: Task Comet Watch Failed\"),\n    #on_success_callback=slack_task(\":ok_hand: Task Comet Watch Success\"),\n    env={\n        'AIRFLOW_ENDPOINT':\"https://airflow.my.server.com/api/experimental\",\n        'COMET_DATASETS':\"/project/data\",\n        'COMET_METADATA':\"/project/metadata\",\n        'COMET_AREA_ACCEPTED':\"working\",\n        'COMET_AREA_PENDING':\"staging\",\n        'COMET_ARCHIVE':\"true\",\n        'COMET_LAUNCHER':\"airflow\",\n        'COMET_HIVE':\"true\",\n        'COMET_ANALYZE':\"true\"\n    },\n    dag=dag)\n")),(0,o.kt)("h3",{id:"ingestion-dag"},"Ingestion DAG"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime, timedelta\nfrom airflow.operators.slack_operator import SlackAPIPostOperator\n\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2018, 11, 2),\n    'email': ['me@here.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG('comet_ingest',max_active_runs=1 , catchup=False, default_args=default_args, schedule_interval = None)\n\ndef slack_task(msg):\n    slack_alert = SlackAPIPostOperator(\n        task_id='slack_alert',\n        channel=\"#airflow\",\n        token=\"xoxp-64071012534-475450904118-524897638692-f9a90d49fd7fb312a574b4570d557b9a\",\n        text = msg,\n        username = 'airflow',)\n    return slack_alert.execute(msg=msg)\n\nCOMET_SPARK_CMD = \"spark2-submit \\\n                        --keytab /etc/keytabs/importhdfs.keytab \\\n                        --principal importhdfs@MY.BIGDATA \\\n                        --conf spark.jars.packages=\\\"\\\" \\\n                        --conf spark.yarn.appMasterEnv.COMET_METADATA=/project/metadata \\\n                        --conf spark.yarn.appMasterEnv.COMET_ACCEPTED=working \\\n                        --conf spark.yarn.appMasterEnv.COMET_DATASETS=/project/data \\\n                        --master yarn \\\n                        --deploy-mode cluster /home/airflow/program/comet-assembly-0.1.jar\"\n\ntemplated_command = COMET_SPARK_CMD + \"\"\" {{ dag_run.conf['command'] }}\"\"\"\n\nCometIngest = BashOperator(\n    task_id='comet_ingest',\n    bash_command=templated_command,\n    #on_failure_callback=slack_task(\":red_circle: Task Comet Ingest Failed: \"),\n    #on_success_callback=slack_task(\":ok_hand: Task Comet Ingest Success: \"),\n    env={\n        'COMET_DATASETS':\"/project/data\",\n        'COMET_METADATA':\"/project/metadata\",\n        'COMET_AREA_ACCEPTED':\"working\",\n        'COMET_AREA_PENDING':\"staging\",\n        'COMET_ARCHIVE':\"true\",\n        'COMET_LAUNCHER':\"airflow\",\n        'COMET_HIVE':\"true\",\n        'COMET_ANALYZE':\"true\"\n    },\n    dag=dag)\n")))}m.isMDXComponent=!0}}]);