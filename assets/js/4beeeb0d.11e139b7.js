(self.webpackChunkcomet_docs=self.webpackChunkcomet_docs||[]).push([[3626],{3905:function(e,t,a){"use strict";a.d(t,{Zo:function(){return c},kt:function(){return d}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),u=p(a),d=r,f=u["".concat(s,".").concat(d)]||u[d]||m[d]||i;return a?n.createElement(f,o(o({ref:t},c),{},{components:a})):n.createElement(f,o({ref:t},c))}));function d(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},2903:function(e,t,a){"use strict";a.r(t),a.d(t,{frontMatter:function(){return o},metadata:function(){return l},toc:function(){return s},default:function(){return c}});var n=a(2122),r=a(9756),i=(a(7294),a(3905)),o={sidebar_position:1,title:"Command Variables"},l={unversionedId:"cli/introduction",id:"cli/introduction",isDocsHomePage:!1,title:"Command Variables",description:"Comet Data Pipeline is written using Scala / Spark and is thus run using the spark-submit command.",source:"@site/docs/cli/introduction.md",sourceDirName:"cli",slug:"/cli/introduction",permalink:"/comet-data-pipeline/docs/cli/introduction",editUrl:"https://github.com/ebiznext/comet-data-pipeline/edit/master/docs/docs/cli/introduction.md",version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Command Variables"},sidebar:"cometSidebar",previous:{title:"Amazon Web Services",permalink:"/comet-data-pipeline/docs/cloud/aws"},next:{title:"import",permalink:"/comet-data-pipeline/docs/cli/import"}},s=[{value:"Comet Data Pipeline Custom conf file",id:"comet-data-pipeline-custom-conf-file",children:[]},{value:"Env vars in cluster mode",id:"env-vars-in-cluster-mode",children:[]},{value:"General Configuration",id:"general-configuration",children:[{value:"Filesystem",id:"filesystem",children:[]}]}],p={toc:s};function c(e){var t=e.components,a=(0,r.Z)(e,["components"]);return(0,i.kt)("wrapper",(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Comet Data Pipeline is written using Scala / Spark and is thus run using the spark-submit command."),(0,i.kt)("p",null,"To run it with the default configuration, you simply launch it as follows :"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"SPARK_HOME/bin/spark-submit --class com.ebiznext.comet.job.Main ../bin/comet-spark3_2.12-VERSION-assembly.jar COMMAND [ARGS]\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"SPARK_HOME: Spark home directory"),(0,i.kt)("li",{parentName:"ul"},"COMMAND: Any of the command described in the CLI section followed by optional arguments"),(0,i.kt)("li",{parentName:"ul"},"ARGS: Option list of command arguments")),(0,i.kt)("h2",{id:"comet-data-pipeline-custom-conf-file"},"Comet Data Pipeline Custom conf file"),(0,i.kt)("p",null,"You may also pass any Spark arguments as usual but also pass a custom ",(0,i.kt)("inlineCode",{parentName:"p"},"application.conf")," file\nusing the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/lightbend/config"},"HOCON")," syntax  that supersedes the default Comet Data Pipeline settings.\ndefault settings are found in the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/ebiznext/comet-data-pipeline/blob/master/src/main/resources/reference.conf"},"reference.conf"),"\nand ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/ebiznext/comet-data-pipeline/blob/master/src/main/resources"},"reference-*.conf")," files. In your ",(0,i.kt)("inlineCode",{parentName:"p"},"application.conf"),"file you only\nneed to redefine the variables you want to customize."),(0,i.kt)("p",null,"Some of those configurations may also be redefined through environment variables. "),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"In client mode: To pass those env vars, simply export / set them before calling spark-submmit."),(0,i.kt)("li",{parentName:"ul"},"In cluster mode, you need to pass them as extra driver options.")),(0,i.kt)("p",null,"Passing the ",(0,i.kt)("inlineCode",{parentName:"p"},"application.conf")," file to the spark job use the syntax below:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'export CUSTOM_OPTIONS="--conf spark.driver.extraJavaOptions=-Dconfig.file=$PWD/application.conf"\nSPARK_HOME/bin/spark-submit $CUSTOM_OPTIONS --class com.ebiznext.comet.job.Main ../bin/comet-spark3_2.12-VERSION-assembly.jar COMMAND [ARGS]\n')),(0,i.kt)("h2",{id:"env-vars-in-cluster-mode"},"Env vars in cluster mode"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"On Premise: To pass Comet Data Pipeline env vars in cluster mode, you'll have to put them in the spark-defaults.conf file or pass them as arguments to your\nSpark job as described in this ",(0,i.kt)("a",{parentName:"p",href:"https://stackoverflow.com/questions/37887168/how-to-pass-environment-variables-to-spark-driver-in-cluster-mode-with-spark-sub"},"article"),"  ")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"On Google Cloud: To make it available for all your jobs, you need to pass them in the ",(0,i.kt)("inlineCode",{parentName:"p"},"DataprocClusterCreateOperator")," using the ",(0,i.kt)("inlineCode",{parentName:"p"},"spark-env:"),"prefix\nas described in the example below:"))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'    create_cluster = DataprocClusterCreateOperator(\n        task_id=\'create_dataproc_cluster\',\n        cluster_name=CLUSTER_NAME,\n        num_workers= \'${dataproc_cluster_size}\',\n        zone=ZONE,\n        region="${region}",\n        tags = ["dataproc"],\n        storage_bucket = "dataproc-${project_id}",\n        image_version=\'2.0.1-debian10\',\n        master_machine_type=MASTER_MACHINE_TYPE,\n        worker_machine_type=WORKER_MACHINE_TYPE,\n        service_account = "${service_account}",\n        internal_ip_only = True,\n        subnetwork_uri = "projects/${project_id}/regions/${region}/subnetworks/${subnet}",\n        properties = {\n            "spark-env:COMET_FS": "gs://${my_bucket}",\n            "spark-env:COMET_HIVE": "false",\n            "spark-env:COMET_GROUPED": "false",\n            "spark-env:COMET_AUDIT_SINK_TYPE": "BigQuerySink"\n            }\n    )\n')),(0,i.kt)("p",null,"In the example above, the variables are available in all the tasks that will be started on this cluster."),(0,i.kt)("p",null,"To set variables for specific tasks only, use a syntax similar to this one:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"t1 = dataproc_operator.DataProcSparkOperator(\n  task_id ='my_task',\n  dataproc_spark_jars='gs://my-bucket/comet-spark3_2.12-VERSION-assembly.jar',\n  cluster_name='cluster',\n  main_class = 'com.ebiznext.comet.job.Main',\n  arguments=['import'],\n  project_id='my-project-id',\n  dataproc_spark_properties={'spark.driver.extraJavaOptions':'-DCOMET_FS=gs://${my_bucket} -DCOMET_HIVE=false -DCOMET_GROUPED=false'},\n  dag=dag)\n")),(0,i.kt)("h2",{id:"general-configuration"},"General Configuration"),(0,i.kt)("h3",{id:"filesystem"},"Filesystem"),(0,i.kt)("p",null,"A filesystem is the location where datasets and Comet Data Pipeline metadata used for ingestion are stored."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"On premise this reference the folder where datasets and metadata are stored, eq.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"On a local filesystem: file://"),(0,i.kt)("li",{parentName:"ul"},"On a HDFS: hdfs://localhost:9000"))),(0,i.kt)("li",{parentName:"ul"},"In the cloud:    ",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"On Google Cloud Platform: gs://my-bucket"),(0,i.kt)("li",{parentName:"ul"},"On Microsoft Azure: abfs://",(0,i.kt)("a",{parentName:"li",href:"mailto:my-bucket@comet.dfs.core.windows.net"},"my-bucket@comet.dfs.core.windows.net")),(0,i.kt)("li",{parentName:"ul"},"On Amazon Web Service: s3a://my_bucket")))),(0,i.kt)("p",null,"Comet Data Pipeline allows you to store datasets and metadata in two different filesystems. Thi is useful if you want to define a specific lifecycle\nfor your datasets."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"left"},"HOCON Variable"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Env variable"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Default"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"file-system"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_FS"),(0,i.kt)("td",{parentName:"tr",align:"left"},"file://"),(0,i.kt)("td",{parentName:"tr",align:"left"},"File system where datasets will be located")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"metadata-file-system"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_METADATA_FS"),(0,i.kt)("td",{parentName:"tr",align:"left"},"${COMET_METADATA_FS} if defined, ${file-system} otherwise"),(0,i.kt)("td",{parentName:"tr",align:"left"},"File system where Comet metadata will be located")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"root"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_ROOT"),(0,i.kt)("td",{parentName:"tr",align:"left"},"/tmp"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Root directory of the datasets and metadata files in the defined filesystem above")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"datasets"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_DATASETS"),(0,i.kt)("td",{parentName:"tr",align:"left"},'${COMET_DATASETS} if defined,  ${root}"/datasets" otherwise'),(0,i.kt)("td",{parentName:"tr",align:"left"},"Folder where datasets are located in the datasets ",(0,i.kt)("inlineCode",{parentName:"td"},"file-system"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"datasets"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_FS"),(0,i.kt)("td",{parentName:"tr",align:"left"},"file://"),(0,i.kt)("td",{parentName:"tr",align:"left"},"File system where datasets will be located")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"file-system"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_FS"),(0,i.kt)("td",{parentName:"tr",align:"left"},"file://"),(0,i.kt)("td",{parentName:"tr",align:"left"},"File system where datasets will be located")))))}c.isMDXComponent=!0}}]);