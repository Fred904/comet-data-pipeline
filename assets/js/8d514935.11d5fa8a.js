(self.webpackChunkcomet_docs=self.webpackChunkcomet_docs||[]).push([[1193],{3905:function(e,t,n){"use strict";n.d(t,{Zo:function(){return u},kt:function(){return d}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var o=a.createContext({}),p=function(e){var t=a.useContext(o),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(o.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,o=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),m=p(n),d=r,g=m["".concat(o,".").concat(d)]||m[d]||c[d]||i;return n?a.createElement(g,l(l({ref:t},u),{},{components:n})):a.createElement(g,l({ref:t},u))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,l=new Array(i);l[0]=m;var s={};for(var o in t)hasOwnProperty.call(t,o)&&(s[o]=t[o]);s.originalType=e,s.mdxType="string"==typeof e?e:r,l[1]=s;for(var p=2;p<i;p++)l[p]=n[p];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},93:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return l},metadata:function(){return s},toc:function(){return o},default:function(){return u}});var a=n(2122),r=n(9756),i=(n(7294),n(3905)),l={sidebar_position:30},s={unversionedId:"reference/transform",id:"reference/transform",isDocsHomePage:!1,title:"Transform",description:"Job",source:"@site/docs/reference/transform.md",sourceDirName:"reference",slug:"/reference/transform",permalink:"/comet-data-pipeline/docs/reference/transform",editUrl:"https://github.com/ebiznext/comet-data-pipeline/edit/master/docs/docs/reference/transform.md",version:"current",sidebarPosition:30,frontMatter:{sidebar_position:30},sidebar:"cometSidebar",previous:{title:"Load",permalink:"/comet-data-pipeline/docs/reference/load"},next:{title:"Extract",permalink:"/comet-data-pipeline/docs/howto/extract"}},o=[{value:"Job",id:"job",children:[]},{value:"Task",id:"task",children:[]},{value:"Partitioning",id:"partitioning",children:[]},{value:"Clustering",id:"clustering",children:[]},{value:"Views",id:"views",children:[]}],p={toc:o};function u(e){var t=e.components,n=(0,r.Z)(e,["components"]);return(0,i.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h2",{id:"job"},"Job"),(0,i.kt)("p",null,"A job is a set of transform tasks executed using the specified engine."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"name: String\n")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Required"),". Job logical name."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"tasks: List[Task]\n")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Required"),". List of transform tasks to execute."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"area : String\n")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Required"),". Area where the data is located."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"When using the BigQuery engine, teh area corresponds to the dataset name we will be working on in this job."),(0,i.kt)("li",{parentName:"ul"},'When using the Spark engine, this is folder where the data should be store. Default value is "business"')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"format: String\n")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Optional"),'. output file format when using Spark engine. Ingored for BigQuery. Default value is "parquet".'),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"coalesce: Boolean\n")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Optional"),". When outputting files, should we coalesce it to a single file. Useful when CSV is the output format."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"udf : String\n")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Optional"),"."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Register UDFs written in this JVM class when using Spark engine"),(0,i.kt)("li",{parentName:"ul"},"Register UDFs stored at this location when using BigQuery engine")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"views : Map[String,String]\n")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Optional"),". Create temporary views using where the key is the view name and the map the SQL request corresponding to this view using the SQL engine supported syntax."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"engine : String\n")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Optional"),". SPARK or BQ. Default value is SPARK."),(0,i.kt)("h2",{id:"task"},"Task"),(0,i.kt)("p",null,"Task executed in the context of a job. Each task is executed in its own session."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"sql: String\n")),(0,i.kt)("p",null,"Main SQL request to exexute (do not forget to prefix table names with the database name to avoid conflicts)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"domain: String\n")),(0,i.kt)("p",null,"Output domain in output Area (Will be the Database name in Hive or Dataset in BigQuery)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"dataset: String\n")),(0,i.kt)("p",null,"Dataset Name in output Area (Will be the Table name in Hive & BigQuery)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"write: String\n")),(0,i.kt)("p",null,"Append to or overwrite existing data"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"area: String\n")),(0,i.kt)("p",null,"Target Area where domain / dataset will be stored."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"partition: List[String]\n")),(0,i.kt)("p",null,"List of columns used for partitioning the outtput."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"presql: List[String]\n")),(0,i.kt)("p",null,"List of SQL requests to executed before the main SQL request is run"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"postsql: List[String]\n")),(0,i.kt)("p",null,"List of SQL requests to executed after the main SQL request is run"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"sink: Sink\n")),(0,i.kt)("p",null,"Where to sink the data"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"rls: List[RowLevelSecurity]\n")),(0,i.kt)("p",null,"Row level security policy to apply too the output data."),(0,i.kt)("h2",{id:"partitioning"},"Partitioning"),(0,i.kt)("h2",{id:"clustering"},"Clustering"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/"},"https://deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/")),(0,i.kt)("h2",{id:"views"},"Views"))}u.isMDXComponent=!0}}]);