(window.webpackJsonp=window.webpackJsonp||[]).push([[28],{128:function(e,t,n){"use strict";n.d(t,"a",(function(){return p})),n.d(t,"b",(function(){return u}));var a=n(0),i=n.n(a);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var c=i.a.createContext({}),b=function(e){var t=i.a.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=b(e.components);return i.a.createElement(c.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return i.a.createElement(i.a.Fragment,{},t)}},d=i.a.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=b(n),d=a,u=p["".concat(l,".").concat(d)]||p[d]||m[d]||r;return n?i.a.createElement(u,o(o({ref:t},c),{},{components:n})):i.a.createElement(u,o({ref:t},c))}));function u(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,l=new Array(r);l[0]=d;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:a,l[1]=o;for(var c=2;c<r;c++)l[c]=n[c];return i.a.createElement.apply(null,l)}return i.a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},98:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return l})),n.d(t,"metadata",(function(){return o})),n.d(t,"toc",(function(){return s})),n.d(t,"default",(function(){return b}));var a=n(3),i=n(7),r=(n(0),n(128)),l={sidebar_position:2},o={unversionedId:"howto/load",id:"howto/load",isDocsHomePage:!1,title:"Load",description:"Load Rules",source:"@site/docs/howto/load.md",sourceDirName:"howto",slug:"/howto/load",permalink:"/comet-data-pipeline/docs/howto/load",editUrl:"https://github.com/ebiznext/comet-data-pipeline/edit/master/docs/docs/howto/load.md",version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"cometSidebar",previous:{title:"Extract",permalink:"/comet-data-pipeline/docs/howto/extract"},next:{title:"transform",permalink:"/comet-data-pipeline/docs/howto/transform"}},s=[{value:"Type Rules",id:"type-rules",children:[]},{value:"Domain Rules",id:"domain-rules",children:[]},{value:"Schema Rules",id:"schema-rules",children:[{value:"Privacy Strategy",id:"privacy-strategy",children:[]}]},{value:"Write Strategy",id:"write-strategy",children:[{value:"Partitioning",id:"partitioning",children:[]},{value:"Compaction",id:"compaction",children:[]}]},{value:"Load Workflow",id:"load-workflow",children:[]},{value:"Import Step",id:"import-step",children:[{value:"How it works",id:"how-it-works",children:[]},{value:"Running it",id:"running-it",children:[]}]},{value:"Watch Step",id:"watch-step",children:[{value:"How it works",id:"how-it-works-1",children:[]},{value:"Running it",id:"running-it-1",children:[]}]},{value:"Ingestion Step",id:"ingestion-step",children:[{value:"How it works",id:"how-it-works-2",children:[]},{value:"Running it",id:"running-it-2",children:[]}]},{value:"Export Step",id:"export-step",children:[{value:"How it works",id:"how-it-works-3",children:[]},{value:"Running it",id:"running-it-3",children:[]}]}],c={toc:s};function b(e){var t=e.components,n=Object(i.a)(e,["components"]);return Object(r.b)("wrapper",Object(a.a)({},c,n,{components:t,mdxType:"MDXLayout"}),Object(r.b)("p",null,"Load Rules"),Object(r.b)("p",null,"Load rules are stored in the folder referenced by the COMET_METADATA\nenvironment variable (/tmp/metadata by default)."),Object(r.b)("blockquote",null,Object(r.b)("p",{parentName:"blockquote"},"\ud83d\udcdd ",Object(r.b)("strong",{parentName:"p"},"You need to export the COMET_METADATA variable before executing any comet load step.")),Object(r.b)("p",{parentName:"blockquote"},Object(r.b)("inlineCode",{parentName:"p"},"export COMET_METADATA=hdfs:///my/metadata"))),Object(r.b)("p",null,"Dataset validation is based on a set of rules we define in schema files.\nSchema files describe how the input files are parsed using a set of rules :"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"Type Rules: Rules that describe the recognized fields formats."),Object(r.b)("li",{parentName:"ul"},"Domain Rules: Rules that describe the file format and load strategy"),Object(r.b)("li",{parentName:"ul"},"Schema Rules: Rules that describe field format using pattern matching"),Object(r.b)("li",{parentName:"ul"},"Assertions:  Rules that must be respected by the whole input file. These rules are executed once the file has been ingested.")),Object(r.b)("h2",{id:"type-rules"},"Type Rules"),Object(r.b)("p",null,"Types are defined in the file $COMET_METADATA/types/types.comet.yml."),Object(r.b)("p",null,"A type is defined by:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},'its name: a string such as "username", "int", "boolean", "long"')),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"the primitive type it is mapped to. Below is the list of all primitive types:"),Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},Object(r.b)("inlineCode",{parentName:"li"},"string")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("inlineCode",{parentName:"li"},"byte"),": single char field"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("inlineCode",{parentName:"li"},"decimal"),": For exact arithmetic. Used for money computation"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("inlineCode",{parentName:"li"},"long"),": integers"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("inlineCode",{parentName:"li"},"double"),": floating numbers"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("inlineCode",{parentName:"li"},"boolean"),": boolean values"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("inlineCode",{parentName:"li"},"date")," : date only fields"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("inlineCode",{parentName:"li"},"timestamp"),": date time fields"))),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"the pattern it should match : A java pattern matching expression that matches the field"),Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},'for types of primitive type "date" or date time, "epoch_milli", "epoch_second" or any predefined or custom date pattern as defined in the ',Object(r.b)("a",{parentName:"li",href:"https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/time/format/DateTimeFormatter.html#BASIC_ISO_DATE"},"Date Time Formatter")," Specification.")))),Object(r.b)("p",null,"For each primitive type, a type is defined by default. These default types are\nlocated in the file $COMET_METADATA/types/default.comet.yml and they may be redefined\nin the file $COMET_METADATA/types/types.comet.yml"),Object(r.b)("p",null,"File ",Object(r.b)("inlineCode",{parentName:"p"},"$COMET_METADATA/types/default.comet.yml")),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-yaml"},'types:\n- name: "string"\n    primitiveType: "string"\n    pattern: ".+"\n    sample: "Hello World"\n    comment: "Any set of chars"\n- name: "byte"\n    primitiveType: "byte"\n    pattern: "."\n    sample: "x"\n    comment: "Any set of chars"\n- name: "date"\n    primitiveType: "date"\n    pattern: "yyyy/MM/dd"\n    sample: "2018/07/21"\n    comment: "Data in the format yyyy/MM/dd"\n- name: "double"\n    primitiveType: "double"\n    pattern: "-?\\\\d*\\\\.{0,1}\\\\d+"\n    sample: "-45.78"\n    comment: "Any floating value"\n- name: "long"\n    primitiveType: "long"\n    pattern: "-?\\\\d+"\n    sample: "-64564"\n    comment: "any positive or negative number"\n- name: "boolean"\n    primitiveType: "boolean"\n    pattern: "(?i)true|yes|[y1]<-TF->(?i)false|no|[n0]"\n    sample: "TruE"\n- name: "timestamp"\n    primitiveType: "timestamp"\n    pattern: "epoch_milli"\n    sample: "1548165436433"\n    comment: "date/time in epoch millis"\n')),Object(r.b)("p",null,'Any semantic type that maps to the boolean primitive type must match against a special regex.\nThis regex is made of two parts separated by the string "<-TF->". values matching the left side will\nbe interpreted as the boolean value "true" and values matching the right side will be interpreted as the boolean value "false".'),Object(r.b)("p",null,"We may add new types that map to these primitive types.\nFor our example above, we will add the following\nsemantic types to allow better validation on the input fields"),Object(r.b)("p",null,"File ",Object(r.b)("inlineCode",{parentName:"p"},"$COMET_METADATA/types/types.comet.yml")),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-yaml"},'\ntypes:\n- name: "email"\n    primitiveType: "string"\n    pattern: "[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\\\\\.[A-Za-z]{2,6}"\n    sample: "me@company.com"\n    comment: "Valid email only"\n- name: "customerid"\n    primitiveType: "string"\n    pattern: "[A-Z][0-9]{6}"\n    sample: "A123456"\n    comment: "Letter followed by 6 digits"\n- name: "sellerid"\n    primitiveType: "string"\n    pattern: "[0-9]{6}"\n    sample: "123456"\n    comment: "6 digits string"\n')),Object(r.b)("p",null,"Now that we have defined the set of semantic\ntypes we want to recognize, we may start defining our schemas."),Object(r.b)("h2",{id:"domain-rules"},"Domain Rules"),Object(r.b)("p",null,'Files are organized by domain. In our example, the "customers" and "orders"\nfiles belong to the "sales" domain  and the "sellers" file belong to the "HR"\ndomain.'),Object(r.b)("p",null,"Domain rules are YAML files located in the folder\n$COMET_METADATA/domains. They defined :"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"The directory where the files coming from this domain are stored"),Object(r.b)("li",{parentName:"ul"},'The ack extension for ack files. "ack" by default.'),Object(r.b)("li",{parentName:"ul"},'Raw file extensions to recognize.  "json", "csv", "dsv", "psv" by default.')),Object(r.b)("p",null,'The load pipeline also automatically recognize compressed files with\nthe extension "tgz", "gz" and "zip". These files are uncompressed in a\ntemporary location and each raw file in the archive is ingested\nif the filename matches a file pattern in one of the schema in the domain,\notherwise the file is moved to the "unsolved" folder under the domain name\nin the cluster.'),Object(r.b)("p",null,"The file below explains it all:"),Object(r.b)("p",null,"File ",Object(r.b)("inlineCode",{parentName:"p"},"$COMET_METADATA/domains/sales.yml")),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-yaml"},'name: "sales"\ndirectory: "/mnt/incoming/sales"\nack: "ack"\nextensions:\n  - "json"\n  - "psv"\n  - "csv"\n  - "dsv"\n')),Object(r.b)("p",null,"Using the default values, the definition above may be shortened to :"),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-yaml"},'name: "sales"\ndirectory: "/mnt/incoming/sales"\n')),Object(r.b)("p",null,'This instruct the Comet Data Pipeline to scan the "/mnt/incoming/sales"\ndirectory and for each file  dataset.ack check for the following files and\ningest it if present :'),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"dataset.tgz"),Object(r.b)("li",{parentName:"ul"},"dataset.zip"),Object(r.b)("li",{parentName:"ul"},"dataset.gz"),Object(r.b)("li",{parentName:"ul"},"dataset.json"),Object(r.b)("li",{parentName:"ul"},"dataset.csv"),Object(r.b)("li",{parentName:"ul"},"dataset.dsv"),Object(r.b)("li",{parentName:"ul"},"dataset.psv")),Object(r.b)("blockquote",null,Object(r.b)("p",{parentName:"blockquote"},"\ud83d\udcdd ",Object(r.b)("strong",{parentName:"p"},"To process files without relying on ack files, simply define the ack attribute with an empty string :`\n`",Object(r.b)("inlineCode",{parentName:"strong"},'ack : ""')))),Object(r.b)("p",null,"To ingest files present in the domain incoming directory (/mnt/incoming/sales),\nwe need to add schema definitions to the domain description file,\naka $COMET_METADATA/domains/sales.yml."),Object(r.b)("p",null,"You can define only one domain per YAML domain definition file."),Object(r.b)("h2",{id:"schema-rules"},"Schema Rules"),Object(r.b)("p",null,"A schema is associated to an incoming file if the filename matches the pattern\ndefined in the schema.\nThe schema hold the parsing rules through metadata describing the file format\nand type mapping rules for each attribute."),Object(r.b)("p",null,'First, we add the schema definition to the "customer" file in the domain definition file'),Object(r.b)("p",null,"File ",Object(r.b)("inlineCode",{parentName:"p"},"$COMET_METADATA/domains/sales.yml")),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-yaml"},'\n    name: "sales"\n    directory: "/mnt/incoming/sales"\n    ack: "ack"\n    extensions:\n      - "json"\n      - "psv"\n      - "csv"\n      - "dsv"\n    schemas:\n      - name: "customers"\n        pattern: "customers-.*.psv"\n        attributes:\n          - name: "id"\n            type: "customerid"\n            required: true\n          - name: "signup"\n            type: "datetime"\n            required: false\n          - name: "contact"\n            type: "email"\n            required: false\n          - name: "birthdate"\n            type: "date"\n            required: false\n          - name: "name1"\n            type: "string"\n            required: false\n            rename: "firstname"\n          - name: "name2"\n            type: "string"\n            required: false\n            rename: "lastname"\n        metadata:\n          mode: "FILE"\n          format: "DSV"\n          withHeader: true\n          separator: "|"\n          quote: "\\""\n          escape: "\\\\"\n          write: "APPEND"\n')),Object(r.b)("p",null,"The schema section in the YAML above should be read as follows :"),Object(r.b)("p",null,".. csv-table:: Schema definition\n:widths: 20, 60"),Object(r.b)("p",null,'   pattern,Filename pattern to match in the domain directory\nname, Schema name: folder where the dataset is stored and Hive table prefix.\nmetadata.mode, always FILE. STREAM is reserved for future use.\nmetadata.format, DSV for delimiter separated values file. SIMPLE_JSON and JSON are also supported.\nmetadata.withHeader, Does the input file has a header\nmetadata.separator, What is the field separator\nmetadata.quote, How are string delimited\nmetadata.escape, How are characters escaped\nmetadata.write, Should we APPEND or OVERWRITE existing data in the  cluster\nmetadata.multiline, "Are JSON object on multiple line. Used when format is JSON or SIMPLE_JSON. This slow down parsing"\nmetadata.array, "Should we treat the file as a single array of JSON objects. Used  when format is JSON or SIMPLE_JSON and the input data is in brackets ',"[...]",'"'),Object(r.b)("blockquote",null,Object(r.b)("p",{parentName:"blockquote"},"\ud83d\udcdd ",Object(r.b)("strong",{parentName:"p"},"Simple JSON are JSON with top level attributes of basic types only. JSON may be used wherever\nyou use SIMPLE_JSON but SIMPLE_JSON will make parsing much faster."))),Object(r.b)("p",null,"Metadata properties may also be defined at the domain level. They will be inherited by all schemas of the domain.\nAny metadata property may be redefined at the attribute level."),Object(r.b)("p",null,"Each field in the input file is defined using by its name, type and privacy level.\nWhen a header is present, fields do not need to be ordered, since Comet uses the field name."),Object(r.b)("p",null,"The attributes section in the YAML above should be read as follows :"),Object(r.b)("table",null,Object(r.b)("thead",{parentName:"table"},Object(r.b)("tr",{parentName:"thead"},Object(r.b)("th",{parentName:"tr",align:null},"Attribute"),Object(r.b)("th",{parentName:"tr",align:null},"definition"))),Object(r.b)("tbody",{parentName:"table"},Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",{parentName:"tr",align:null},"name"),Object(r.b)("td",{parentName:"tr",align:null},"Field name as specified in the header. If no header is present, this willthe field name in the ingested dataset.")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",{parentName:"tr",align:null},"type"),Object(r.b)("td",{parentName:"tr",align:null},"Type as defined in the Type Rules section above.")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",{parentName:"tr",align:null},"required"),Object(r.b)("td",{parentName:"tr",align:null},"Can this field be empty ?")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",{parentName:"tr",align:null},"privacy"),Object(r.b)("td",{parentName:"tr",align:null},"How should this field be altered during parsing. May be used to transform the output value.")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",{parentName:"tr",align:null},"rename"),Object(r.b)("td",{parentName:"tr",align:null},"When header is present in DSV files, this is the new field name in the ingested dataset")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",{parentName:"tr",align:null},"metricType"),Object(r.b)("td",{parentName:"tr",align:null},"When statistics generation is requested, should this field be treated as continuous, discrete or text value ? Valid values are CONTINUOUS, DISCRETE, TEXT, NONE")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",{parentName:"tr",align:null},"array"),Object(r.b)("td",{parentName:"tr",align:null},"true when this attribute is an array, false by default")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",{parentName:"tr",align:null},"script"),Object(r.b)("td",{parentName:"tr",align:null},"Allows you to add a new field computed from a UDF or a Spark SQL built-in standard function")))),Object(r.b)("h3",{id:"privacy-strategy"},"Privacy Strategy"),Object(r.b)("p",null,"Default valid values are NONE, HIDE, MD5, SHA1, SHA256, SHA512, AES(not implemented).\nCustom values may also be defined by adding a new privacy option in the application.conf. The default reference.conf file defines the following valid privacy\nstrategies:"),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-hocon"},'privacy {\n  options = {\n    "none": "com.ebiznext.comet.privacy.No",\n    "hide": "com.ebiznext.comet.privacy.Hide",\n    "hide10X": "com.ebiznext.comet.privacy.Hide(\\"X\\",10)",\n    "approxLong20": "com.ebiznext.comet.privacy.ApproxLong(20)",\n    "md5": "com.ebiznext.comet.privacy.Md5",\n    "sha1": "com.ebiznext.comet.privacy.Sha1",\n    "sha256": "com.ebiznext.comet.privacy.Sha256",\n    "sha512": "com.ebiznext.comet.privacy.Sha512",\n    "initials": "com.ebiznext.comet.privacy.Initials"\n  }\n}\n')),Object(r.b)("p",null,"Any new privacy strategy should implement the following trait :"),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-scala"},"trait Encryption {\n  def encrypt(s: String): String\n}\n")),Object(r.b)("p",null,"Below, the complete domain definition files."),Object(r.b)("p",null,"File ",Object(r.b)("inlineCode",{parentName:"p"},"$COMET_METADATA/domains/sales.yml")),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-yaml"},'    name: "sales"\n    directory: "/mnt/incoming/sales"\n    metadata:\n      mode: "FILE"\n      format: "DSV"\n      withHeader: true\n      quote: "\\""\n      escape: "\\\\"\n      write: "APPEND"\n    schemas:\n      - name: "customers"\n        pattern: "customers-.*.psv"\n        metadata:\n          separator: "|"\n        attributes:\n          - name: "id"\n            type: "customerid"\n            required: true\n          - name: "signup"\n            type: "datetime"\n            required: false\n          - name: "contact"\n            type: "email"\n            required: false\n          - name: "birthdate"\n            type: "date"\n            required: false\n          - name: "name1"\n            type: "string"\n            required: false\n            rename: "firstname"\n          - name: "name2"\n            type: "string"\n            required: false\n            rename: "lastname"\n      - name: "orders"\n        pattern: "orders-.*.csv"\n        merge:\n          key:\n            - "id"\n          delete: "customer_id is null"\n        metadata:\n          separator: ","\n        attributes:\n          - name: "order_id"\n            type: "string"\n            required: true\n            rename: "id"\n          - name: "customer_id"\n            type: "customerid"\n            required: false\n          - name: "amount"\n            type: "decimal"\n            required: false\n          - name: "seller_id"\n            type: "string"\n            required: false\n')),Object(r.b)("blockquote",null,Object(r.b)("p",{parentName:"blockquote"},"\ud83d\udcdd ",Object(r.b)("strong",{parentName:"p"},"The merge attribute above should be read as follows:"))),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-yaml"},'merge:\n key:\n   - "id"\n delete: "customer_id is null"\n')),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},'When a new orders dataset is imported, only the last occurrence of the record identified by the key column "id" should be kept'),Object(r.b)("li",{parentName:"ul"},"and any record imported with a null column_id should be removed from the existing dataset.")),Object(r.b)("p",null,"File ",Object(r.b)("inlineCode",{parentName:"p"},"$COMET_METADATA/domains/hr.yml")),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-yaml"},'name: "hr"\ndirectory: "/mnt/incoming/hr"\nmetadata:\n  mode: "FILE"\n  format: "JSON"\nschemas:\n  - name: "sellers"\n    pattern: "sellers-.*.json"\n    metadata:\n      array: true\n      format: "SIMPLE_JSON"\n      write: "APPEND"\n    attributes:\n      - name: "id"\n        type: "string"\n        required: true\n      - name: "seller_email"\n        type: "email"\n        required: true\n      - name: "location_id"\n        type: "long"\n        required: true\n  - name: "locations"\n    pattern: "locations-.*.json"\n    metadata:\n      format: "JSON"\n      write: "OVERWRITE"\n    attributes:\n      - name: "id"\n        type: "string"\n        required: true\n      - name: "address"\n        type: "struct"\n        required: true\n        attributes:\n          - name: "city"\n            type: "string"\n            required: true\n          - name: "stores"\n            type: "string"\n            array: true\n            required: false\n          - name: "country"\n            type: "string"\n            required: true\n')),Object(r.b)("h2",{id:"write-strategy"},"Write Strategy"),Object(r.b)("h3",{id:"partitioning"},"Partitioning"),Object(r.b)("p",null,"You may control partitioning strategy and tell Comet how you wish to partition your\ndata on disk by specifying one or more attributes in the input file as partition columns."),Object(r.b)("p",null,"If you want to use ingestion date/time as partition columns, you can use predefined attributes\n",Object(r.b)("inlineCode",{parentName:"p"},"year"),", ",Object(r.b)("inlineCode",{parentName:"p"},"month")," ",Object(r.b)("inlineCode",{parentName:"p"},"day"),", ",Object(r.b)("inlineCode",{parentName:"p"},"hour"),", ",Object(r.b)("inlineCode",{parentName:"p"},"minute")," prefixed by ",Object(r.b)("inlineCode",{parentName:"p"},"comet_"),". These columns will\nappear as regular attributes in the resulting dataset and without the prefix ",Object(r.b)("inlineCode",{parentName:"p"},"comet_")),Object(r.b)("p",null,"Below an example of how to partition by ingestion year, month and day."),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-yaml"},'- metadata:\npartition:\n    attributes:\n      - "comet_year"\n      - "comet_month"\n      - "comet_day"\n')),Object(r.b)("h3",{id:"compaction"},"Compaction"),Object(r.b)("p",null,"When saving files as parquet or orc or whatever, the optimal number of partitions depend on the dataset size,\nnumber of records, the size of each record and the block size."),Object(r.b)("p",null,"The goal is to optimise the number of partitions during the write phase."),Object(r.b)("p",null,"You have 3 choices available :"),Object(r.b)("h4",{id:"solution-1--naive-compaction"},"Solution 1 : Naive Compaction"),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},"Save the file in a temporary location"),Object(r.b)("li",{parentName:"ol"},"Get the dataset size."),Object(r.b)("li",{parentName:"ol"},"Divide the dataset size by the  block size to get the number of partitions"),Object(r.b)("li",{parentName:"ol"},"Save the dataset to the target location with the computed number of partitions")),Object(r.b)("p",null,"The main drawback of this approach is that we need to save the file twice."),Object(r.b)("h4",{id:"solution-2--sampling"},"Solution 2 : Sampling"),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},"Get a percentage of the records in the dataframe before saving it."),Object(r.b)("li",{parentName:"ol"},"Save it to a temporary location"),Object(r.b)("li",{parentName:"ol"},"Estimate the size of the final dataset on HDFS based on the size of the sample"),Object(r.b)("li",{parentName:"ol"},"Compute the number of partitions based on this estimation"),Object(r.b)("li",{parentName:"ol"},"Save the dataset to the target location with the computed number of partitions")),Object(r.b)("p",null,"The Naive solution is in fact identical to the Sampling one with a sampling percentage of 100%."),Object(r.b)("h4",{id:"solution-3--absolute-compaction"},"Solution 3 : Absolute Compaction"),Object(r.b)("p",null,"The number of partitions is defined by the user at the schema level."),Object(r.b)("p",null,"Example :"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"0.0 => Means no optimisation.")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"1.0 => Na\xefve Compaction")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"Any integer between 1 ... Int.max => Absolute number of partitions"))),Object(r.b)("p",null,"Below an example of compaction based on a sampling of 20%"),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-yaml"},'- metadata:\npartition:\n    sampling: 0.2 # compute number of partitions based on the size on disk of a sampling of 20% of the dataset\n    attributes:\n      - "comet_year"\n      - "comet_month"\n      - "comet_day"\n')),Object(r.b)("p",null,"With the types catalog, file schemas and save strategy defined we are ready to ingest"),Object(r.b)("h2",{id:"load-workflow"},"Load Workflow"),Object(r.b)("p",null,"The ingestion process follows the steps below :"),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},"Import Step : Files are imported to the cluster in the pending area."),Object(r.b)("li",{parentName:"ol"},"Watch Step : Files in the pending area are submitted for ingestion to the Job Manager (Airflow for example)."),Object(r.b)("li",{parentName:"ol"},"Ingestion Step: Files are validated and converted to a cluster defined file format (parquet, orc ...) and saved as Hive tables.")),Object(r.b)("p",null,"Before running the steps below, please configure first the environment variables\nas described in the Configuration section."),Object(r.b)("h2",{id:"import-step"},"Import Step"),Object(r.b)("h3",{id:"how-it-works"},"How it works"),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},"On startup, all the domain definitions files are loaded from the folder /tmp/metadata/domains"),Object(r.b)("li",{parentName:"ol"},"Directories referenced by the ",Object(r.b)("inlineCode",{parentName:"li"},"directory")," attribute in the YAML domain definition files are scanned for incoming files. The incoming folder must be accessible locally or through a mount point."),Object(r.b)("li",{parentName:"ol"},"Any compressed file or file with any default extension or with one of the extension defined by the ",Object(r.b)("inlineCode",{parentName:"li"},"extensions")," attribute are moved to the cluster in the domain pending folder, /tmp/datasets/pending/",Object(r.b)("inlineCode",{parentName:"li"},"DOMAIN NAME"),"/ by default.")),Object(r.b)("h3",{id:"running-it"},"Running it"),Object(r.b)("p",null,"To run the import step, you have to have the spark & hadoop\nclient libraries in your classpath. You may get them automatically\nby running the import step with the spark-submit command:"),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-shell"},"$SPARK_HOME/bin/spark-submit comet-assembly-VERSION.jar import\n")),Object(r.b)("h2",{id:"watch-step"},"Watch Step"),Object(r.b)("h3",{id:"how-it-works-1"},"How it works"),Object(r.b)("p",null,"The Watch process will scan the all domain pending folders in the cluster.\nAny file that matches the pattern defined by the ",Object(r.b)("inlineCode",{parentName:"p"},"pattern")," attribute in\nthe schema section of the domain definition file is submitted to the Job Manager.\nFiles that do not match any pattern are moved to the unresolved\nfolder, /tmp/datasets/unresolved/",Object(r.b)("inlineCode",{parentName:"p"},"DOMAIN NAME"),"/ by default."),Object(r.b)("p",null,"Once copied to the pending folder, a request for ingestion (see step below) is submitted to the Job Manager."),Object(r.b)("blockquote",null,Object(r.b)("p",{parentName:"blockquote"},"\ud83d\udcdd ",Object(r.b)("strong",{parentName:"p"},"By default the ",Object(r.b)("inlineCode",{parentName:"strong"},"simple")," job manager is invoked. This simple manager\nused for debugging & testing purpose would launch the ingestion step inside the current process.\nIn production, you would configure a job manager running on your cluster.\nComet comes with the ",Object(r.b)("inlineCode",{parentName:"strong"},"airflow")," job manager and sample DAGs required to run all three steps."))),Object(r.b)("h3",{id:"running-it-1"},"Running it"),Object(r.b)("p",null,"To run the import step, you have to have the spark & hadoop\nclient libraries in your classpath. You may get them automatically\nby running the watch step with the spark-submit command:"),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-shell"},"$SPARK_HOME/bin/spark-submit comet-assembly-VERSION.jar watch\n")),Object(r.b)("h2",{id:"ingestion-step"},"Ingestion Step"),Object(r.b)("h3",{id:"how-it-works-2"},"How it works"),Object(r.b)("p",null,"Unlike the steps above, this step does not scan any folder.\nIt takes as its parameters the domain name, schema name and\nfull path of the file that need to be ingested. That's why it is usually\ninvoked through request submitted to a job manager by at the Watch Step."),Object(r.b)("h3",{id:"running-it-2"},"Running it"),Object(r.b)("p",null,"To interactively run it, copy the input file in the pending area\nof a domain and run it as follows:"),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-shell"},"$ SPARK_HOME/bin/spark-submit comet-assembly-VERSION.jar ingest DOMAIN_NAME SCHEMA_NAME hdfs://datasets/domain/pending/file.dsv\n")),Object(r.b)("h2",{id:"export-step"},"Export Step"),Object(r.b)("h3",{id:"how-it-works-3"},"How it works"),Object(r.b)("p",null,"This step is conecerned with exporting the dataset to Elasticsearch / SQl Database / csv or json file\nIt takes as its parameters the domain name, schema name and\nfull path of the file that need to be ingested. That's why it is usually\ninvoked through request submitted to a job manager by at the Watch Step."),Object(r.b)("h3",{id:"running-it-3"},"Running it"),Object(r.b)("p",null,"To interactively run it, copy the input file in the pending area\nof a domain and run it as follows:"),Object(r.b)("pre",null,Object(r.b)("code",{parentName:"pre",className:"language-shell"},"$ SPARK_HOME/bin/spark-submit comet-assembly-VERSION.jar ingest DOMAIN_NAME SCHEMA_NAME hdfs://datasets/domain/pending/file.dsv\n")))}b.isMDXComponent=!0}}]);